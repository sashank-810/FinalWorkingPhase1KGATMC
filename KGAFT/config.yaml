# Model and Tokenizer configuration
model_name_or_path: "Qwen/Qwen3-4B-Thinking-2507" # Strong reasoning model recommended in the doc [cite: 18]
use_flash_attention_2: false # Use Flash Attention for speed and memory optimization

# Dataset configuration
# AFTER
# Dataset configuration
train_dataset_path: "data/train.jsonl"
validation_dataset_path: "data/validation.jsonl"
test_dataset_path: "data/test.jsonl" # For the evaluation script

# Training parameters
output_dir: "./results_qwen4b_finetuned3"
num_train_epochs: 30 # Small datasets often require only 1-3 epochs [cite: 92]
per_device_train_batch_size: 4 # Small batch size to fit in GPU memory [cite: 94]
gradient_accumulation_steps: 4 # Effective batch size will be 4 * 4 = 16 [cite: 94]
learning_rate: 2e-5 # A common starting point for fine-tuning [cite: 92]
optim: "paged_adamw_8bit" # Memory-efficient optimizer
fp16: true # Use mixed precision training [cite: 15, 95]
logging_steps: 25
save_strategy: "epoch"
evaluation_strategy: "epoch"

# PEFT (LoRA) configuration [cite: 82]
lora_r: 4                  # LoRA rank (a small value like 8 is often sufficient)
lora_alpha: 8            # LoRA alpha
lora_dropout: 0.1
# Target modules are model-specific. For Phi-2, these are common choices.
lora_target_modules:
  - "qkv_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"